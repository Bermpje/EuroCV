"""LinkedIn PDF extraction functionality.

This extractor is optimized for LinkedIn-generated PDF exports.
It recognizes LinkedIn-specific formatting and structures.
"""

import re
from datetime import date
from pathlib import Path
from typing import Any, Optional

import fitz  # PyMuPDF
from pdfminer.high_level import extract_text as pdfminer_extract_text
from pdfminer.layout import LAParams

from eurocv.core.extract.base_extractor import ResumeExtractor
from eurocv.core.models import (
    Certification,
    Education,
    Language,
    PersonalInfo,
    Resume,
    Skill,
    WorkExperience,
)


class LinkedInPDFExtractor(ResumeExtractor):
    """Extract text and structure from LinkedIn PDF exports.

    This extractor is specifically optimized for PDFs exported from LinkedIn,
    with specialized parsing for LinkedIn's specific formatting patterns.
    """

    def __init__(self, use_ocr: bool = False):
        """Initialize extractor.

        Args:
            use_ocr: Whether to use OCR for scanned PDFs (requires pytesseract)
        """
        self.use_ocr = use_ocr

    @property
    def name(self) -> str:
        """Return extractor name."""
        return "LinkedIn PDF"

    def can_handle(self, file_path: str) -> bool:
        """Check if this extractor can handle the file.

        Detects LinkedIn PDFs by checking metadata and content for LinkedIn markers.

        Args:
            file_path: Path to the file

        Returns:
            True if file appears to be a LinkedIn PDF export
        """
        if not file_path.lower().endswith(".pdf"):
            return False

        try:
            with fitz.open(file_path) as doc:
                # Check metadata for LinkedIn indicators
                if doc.metadata:
                    producer = doc.metadata.get("producer", "").lower()
                    creator = doc.metadata.get("creator", "").lower()
                    if "linkedin" in producer or "linkedin" in creator:
                        return True

                # Check first page content for LinkedIn markers
                if len(doc) > 0:
                    first_page_text = doc[0].get_text().lower()
                    if (
                        "linkedin.com" in first_page_text
                        or "generated by linkedin" in first_page_text
                    ):
                        return True

        except Exception:
            # If we can't open/read the PDF, we can't handle it
            pass

        return False

    def extract(self, file_path: str) -> Resume:
        """Extract resume data from PDF.

        Args:
            file_path: Path to PDF file

        Returns:
            Resume object with extracted data
        """
        path = Path(file_path)
        if not path.exists():
            raise FileNotFoundError(f"PDF file not found: {file_path}")

        # Try PyMuPDF first (better for most PDFs)
        try:
            text, metadata = self._extract_with_pymupdf(str(path))
        except Exception:
            # Fallback to pdfminer.six
            text, metadata = self._extract_with_pdfminer(str(path))

        # Parse the extracted text into structured data
        resume = self._parse_text_to_resume(text, metadata)
        resume.raw_text = text
        resume.metadata.update(metadata)

        return resume

    def _extract_with_pymupdf(self, file_path: str) -> tuple[str, dict[str, Any]]:
        """Extract text using PyMuPDF.

        Args:
            file_path: Path to PDF file

        Returns:
            Tuple of (text content, metadata dict)
        """
        doc = fitz.open(file_path)
        text_parts = []
        metadata = {
            "page_count": len(doc),
            "format": "PDF",
            "extractor": "pymupdf",
        }

        # Extract metadata
        if doc.metadata:
            metadata.update(
                {
                    "title": doc.metadata.get("title"),
                    "author": doc.metadata.get("author"),
                    "subject": doc.metadata.get("subject"),
                    "keywords": doc.metadata.get("keywords"),
                }
            )

        # Extract text from each page
        for page_num, page in enumerate(doc, 1):
            # Check if page is likely scanned (no text)
            page_text = page.get_text()

            if not page_text.strip() and self.use_ocr:
                # Use OCR for scanned pages
                page_text = self._ocr_page(page)

            text_parts.append(page_text)

        doc.close()

        return "\n\n".join(text_parts), metadata

    def _extract_with_pdfminer(self, file_path: str) -> tuple[str, dict[str, Any]]:
        """Extract text using pdfminer.six.

        Args:
            file_path: Path to PDF file

        Returns:
            Tuple of (text content, metadata dict)
        """
        laparams = LAParams(
            line_margin=0.5,
            word_margin=0.1,
            char_margin=2.0,
            boxes_flow=0.5,
        )

        text = pdfminer_extract_text(file_path, laparams=laparams)

        metadata = {
            "format": "PDF",
            "extractor": "pdfminer",
        }

        return text, metadata

    def _ocr_page(self, page: fitz.Page) -> str:
        """OCR a page using Tesseract.

        Args:
            page: PyMuPDF page object

        Returns:
            Extracted text
        """
        try:
            import io

            import pytesseract
            from PIL import Image

            # Convert page to image
            pix = page.get_pixmap(dpi=300)
            img_data = pix.tobytes("png")
            img = Image.open(io.BytesIO(img_data))

            # Run OCR
            text = pytesseract.image_to_string(img, lang="eng+nld")
            return text
        except ImportError:
            # OCR dependencies not installed
            return ""

    def _parse_text_to_resume(self, text: str, metadata: dict[str, Any]) -> Resume:
        """Parse extracted text into structured Resume.

        This is a heuristic-based parser. For better results, consider:
        - Training a custom NER model
        - Using a pre-trained model for resume parsing
        - Implementing layout analysis

        Args:
            text: Extracted text
            metadata: Document metadata

        Returns:
            Resume object
        """
        resume = Resume()

        # Extract personal info
        resume.personal_info = self._extract_personal_info(text)

        # LinkedIn PDFs have sidebar content at the beginning
        # Extract sidebar content first (Top Skills, Languages, Certifications)
        sidebar_skills = self._extract_sidebar_skills(text)
        sidebar_certs = self._extract_sidebar_certifications(text)

        # Extract sections
        sections = self._split_into_sections(text)

        # Extract work experience
        if "experience" in sections or "work" in sections:
            work_section = sections.get("experience") or sections.get("work", "")
            resume.work_experience = self._extract_work_experience(work_section)

        # Extract education
        if "education" in sections:
            resume.education = self._extract_education(sections["education"])

        # Extract languages (check section first, then full text)
        if "language" in sections:
            resume.languages = self._extract_languages(sections["language"])
        else:
            # Try extracting from full text (languages might be in sidebar)
            resume.languages = self._extract_languages(text)

        # Extract skills - combine sidebar skills with section skills
        section_skills = []
        if "skill" in sections:
            section_skills = self._extract_skills(sections["skill"])

        # Also look for "Specialisms" section for additional skills
        if "summary" in sections or "profile" in sections:
            summary_text = sections.get("summary") or sections.get("profile", "")
            if "specialism" in summary_text.lower():
                specialism_skills = self._extract_specialisms(summary_text)
                section_skills.extend(specialism_skills)

        # Combine sidebar and section skills, removing duplicates
        all_skills = sidebar_skills + section_skills
        seen_skills = set()
        unique_skills = []
        for skill in all_skills:
            normalized = skill.name.lower().replace(" ", "")
            if normalized not in seen_skills:
                seen_skills.add(normalized)
                unique_skills.append(skill)
        resume.skills = unique_skills

        # Extract certifications - combine sidebar and section
        section_certs = []
        if "certification" in sections:
            section_certs = self._extract_certifications(sections["certification"])

        # Combine and deduplicate
        all_certs = sidebar_certs + section_certs
        seen_certs = set()
        unique_certs = []
        for cert in all_certs:
            normalized = cert.name.lower().replace(" ", "")
            if normalized not in seen_certs:
                seen_certs.add(normalized)
                unique_certs.append(cert)
        resume.certifications = unique_certs

        # Extract summary
        if "summary" in sections or "profile" in sections:
            resume.summary = sections.get("summary") or sections.get("profile")

        return resume

    def _extract_sidebar_skills(self, text: str) -> list[Skill]:
        """Extract skills from LinkedIn sidebar 'Top Skills' section.

        Args:
            text: Full resume text

        Returns:
            List of Skill objects
        """
        skills = []

        # Look for "Top Skills" header in first ~500 chars (sidebar)
        match = re.search(
            r"Top Skills\s*\n(.*?)(?=\n\w+\n|Languages|Certifications|$)",
            text[:1000],
            re.IGNORECASE | re.DOTALL,
        )

        if match:
            skills_text = match.group(1)
            # Split by newlines
            lines = [line.strip() for line in skills_text.split("\n") if line.strip()]

            for line in lines:
                # Skip section headers
                if line.lower() in [
                    "contact",
                    "languages",
                    "certifications",
                    "summary",
                ]:
                    break

                # Skip URLs and email
                if "http" in line.lower() or "@" in line or "www." in line.lower():
                    continue

                # Skip if too short or too long
                if len(line) < 3 or len(line) > 50:
                    continue

                skills.append(Skill(name=line))

        return skills

    def _extract_sidebar_certifications(self, text: str) -> list[Certification]:
        """Extract certifications from LinkedIn sidebar.

        Args:
            text: Full resume text

        Returns:
            List of Certification objects
        """
        certifications = []

        # Look for "Certifications" header in first ~1000 chars (sidebar)
        # Stop at the person's name which is typically "FirstName LastName" pattern
        match = re.search(
            r"Certifications?\s*\n(.*?)(?=\n[A-Z][a-z]{2,15}\s+[A-Z][a-z]{2,15}\n.*?(?:Architect|Manager|Developer|Engineer|Consultant))",
            text[:1200],
            re.IGNORECASE | re.DOTALL,
        )

        if match:
            certs_text = match.group(1)
            lines = [line.strip() for line in certs_text.split("\n") if line.strip()]

            # Certifications can span multiple lines
            # Lines that are NOT cert names: short connector words, numbers only, etc
            current_cert = []
            for line in lines:
                # Stop if we clearly hit the name/title section
                # (person's name followed by job title is a strong indicator)
                if re.match(r"^[A-Z][a-z]+\s+[A-Z][a-z]+$", line):
                    # This looks like a person's name
                    if current_cert:
                        cert_name = " ".join(current_cert)
                        certifications.append(Certification(name=cert_name))
                    break

                # Skip empty lines - they separate certifications
                if not line:
                    if current_cert:
                        cert_name = " ".join(current_cert)
                        certifications.append(Certification(name=cert_name))
                        current_cert = []
                    continue

                # Skip lines that are clearly section headers
                if line.lower() in ["summary", "experience", "education", "contact"]:
                    break

                # Accumulate lines that belong to same certification
                current_cert.append(line)

            # Don't forget last cert
            if current_cert:
                cert_name = " ".join(current_cert)
                certifications.append(Certification(name=cert_name))

        return certifications

    def _extract_specialisms(self, text: str) -> list[Skill]:
        """Extract skills from 'Specialisms' section.

        Args:
            text: Summary/profile text that may contain specialisms

        Returns:
            List of Skill objects
        """
        skills = []

        # Look for "Specialism" section
        match = re.search(
            r"Specialism[s]?\s*\n(.*?)(?=\n\n|Experience|$)",
            text,
            re.IGNORECASE | re.DOTALL,
        )

        if match:
            specialisms_text = match.group(1)
            # Split by bullet points or newlines
            lines = re.split(r"[•\n]", specialisms_text)

            for line in lines:
                line = line.strip()

                # Skip if too short
                if len(line) < 5:
                    continue

                # Skip headers
                if line.lower() in ["experience", "education"]:
                    break

                # Clean up the line (remove leading symbols)
                line = re.sub(r"^[•\-\*]\s*", "", line)

                if line:
                    skills.append(Skill(name=line))

        return skills

    def _extract_personal_info(self, text: str) -> PersonalInfo:
        """Extract personal information from text.

        Args:
            text: Resume text

        Returns:
            PersonalInfo object
        """
        info = PersonalInfo()

        # Extract email
        email_pattern = r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b"
        email_matches = re.findall(email_pattern, text)
        if email_matches:
            info.email = email_matches[0]

        # Extract phone (A3: Enhanced patterns)
        phone_patterns = [
            # International with (0) notation: +31 (0)6 12345678
            r"\+\d{1,3}\s*\(0\)\s*\d{1,3}\s*\d{6,8}",
            # International standard: +31-6-12345678, +31 6 12345678
            r"\+\d{1,3}[-\s]?\d{1,3}[-\s]?\d{6,8}",
            # Dutch mobile: 06-12345678, 0612345678
            r"0\d{1}[-\s]?\d{8}",
            # Dutch landline with area code: (020) 1234567, 020-1234567
            r"\(?\d{2,4}\)?[-\s]?\d{6,7}",
            # US format: (555) 123-4567, +1 (555) 123-4567
            r"\+?1?\s*\(?\d{3}\)?[-\s]?\d{3}[-\s]?\d{4}",
            # UK format: +44 20 1234 5678
            r"\+44\s*\d{2,4}\s*\d{4}\s*\d{4}",
            # Generic international
            r"[\+]?[(]?[0-9]{1,4}[)]?[-\s\.]?[(]?[0-9]{1,4}[)]?[-\s\.]?[0-9]{1,4}[-\s\.]?[0-9]{1,9}",
        ]

        # Look in first 500 chars for phone in header/contact section
        phone_matches = []
        for pattern in phone_patterns:
            matches = re.findall(pattern, text[:500])
            phone_matches.extend(matches)

        if phone_matches:
            # Filter out years (4 digits only) and validate length
            valid_phones = []
            for p in phone_matches:
                clean_phone = (
                    p.replace(" ", "")
                    .replace("-", "")
                    .replace(".", "")
                    .replace("(", "")
                    .replace(")", "")
                )
                # Skip if it's just a 4-digit year
                if re.match(r"^\d{4}$", clean_phone):
                    continue
                # Keep if it's a reasonable phone length (6-15 digits)
                if 6 <= len(re.sub(r"\D", "", clean_phone)) <= 15:
                    valid_phones.append(p)

            if valid_phones:
                info.phone = valid_phones[0]

        # Extract name (improved heuristic)
        info.first_name, info.last_name = self._extract_name(text)

        # Extract location from header
        info.city, info.country = self._extract_location_from_header(text)

        return info

    def _extract_name(self, text: str) -> tuple[str, str]:
        """Extract person name from text using improved heuristics.

        Args:
            text: Resume text

        Returns:
            Tuple of (first_name, last_name)
        """
        lines = text.split("\n")
        candidates = []

        # Common sidebar headings and phrases to skip
        sidebar_headings = [
            "contact",
            "top skills",
            "skills",
            "languages",
            "certifications",
            "certificates",
            "summary",
            "profile",
            "experience",
            "education",
            "expertise",
            "competencies",
            "about",
            "honors",
            "awards",
            "other languages",
            "spoken english",
            "native or",
            "limited working",
        ]

        for i, line in enumerate(lines[:30]):  # Check first 30 lines
            line = line.strip()

            # Skip empty lines
            if not line:
                continue

            # Skip common section headings
            if line.lower() in sidebar_headings:
                continue

            # Skip lines with URLs or common non-name patterns
            skip_patterns = [
                r"www\.",
                r"http",
                r"\.com",
                r"\.nl",
                r"\.org",
                r"linkedin",
                r"\(.*\)",  # Text in parentheses
                r"@",  # Email
                r"\d{4}",  # Years
                r"page\s+\d+",  # Page numbers
                r"&",  # Ampersands (often in titles)
                r"\|",  # Pipes (often in titles)
            ]

            if any(
                re.search(pattern, line, re.IGNORECASE) for pattern in skip_patterns
            ):
                continue

            # Check if line looks like a name
            words = line.split()

            # Name should be exactly 2-3 words, each capitalized
            if 2 <= len(words) <= 3:
                # Check if all words are title case and mostly alpha
                if all(
                    word[0].isupper()
                    and word.replace("-", "").replace("'", "").isalpha()
                    for word in words
                    if word
                ):
                    # Calculate a score for this candidate
                    score = 0

                    # Strongly prefer exactly 2 words (First Last)
                    if len(words) == 2:
                        score += 10
                    elif len(words) == 3:
                        score += 5

                    # Prefer short words (first and last names are usually short)
                    if all(3 <= len(word) <= 15 for word in words):
                        score += 5

                    # Prefer lines that are standalone (not too much around them)
                    if len(line) < 30:
                        score += 3

                    # STRONGLY prefer lines 20-25 (typical name position in LinkedIn)
                    if 20 <= i <= 25:
                        score += 20  # Strong bonus for likely name position
                    elif i >= 15:
                        score += 4  # Prefer middle section

                    # Check if words look like common first names (heuristic: ends with common suffixes)
                    first_word = words[0]
                    # Common name endings
                    if any(
                        first_word.endswith(suffix)
                        for suffix in ["el", "an", "en", "on", "er", "le", "ie"]
                    ):
                        score += 2

                    candidates.append((score, words, i, line))

        # Sort by score and pick best candidate
        if candidates:
            candidates.sort(reverse=True)
            best_words = candidates[0][1]

            first_name = best_words[0]
            last_name = " ".join(best_words[1:])
            return first_name, last_name

        return None, None

    def _extract_location_from_header(self, text: str) -> tuple[str, str]:
        """Extract location (city, country) from resume header (A4: Enhanced).

        Args:
            text: Resume text

        Returns:
            Tuple of (city, country)
        """
        # Common location patterns in headers
        # Format: "City, Region, Country" or "City, Country"
        lines = text.split("\n")

        # Check first 50 lines for location patterns
        for line in lines[:50]:
            line = line.strip()

            # Skip empty lines and URLs
            if not line or "http" in line.lower() or "@" in line:
                continue

            # Handle "Area" patterns (e.g., "Amsterdam Area", "Greater London")
            area_match = re.search(
                r"([\w\s]+)\s+(Area|Greater|Region)", line, re.IGNORECASE
            )
            if area_match:
                city = area_match.group(1).strip()
                if len(city) > 2 and len(city) < 30:
                    return city, None

            # Handle "Remote -" patterns (e.g., "Remote - Netherlands")
            remote_match = re.search(r"Remote\s*[-–]\s*([\w\s]+)", line, re.IGNORECASE)
            if remote_match:
                location = remote_match.group(1).strip()
                # Could be a country
                countries = [
                    "Netherlands",
                    "Germany",
                    "Belgium",
                    "France",
                    "United Kingdom",
                    "UK",
                    "United States",
                    "USA",
                ]
                for country in countries:
                    if country.lower() in location.lower():
                        return "Remote", country

            # Look for lines with comma-separated location info
            if "," in line:
                # Common country names and variations (A4: Expanded)
                countries = [
                    "Netherlands",
                    "Holland",
                    "Germany",
                    "Belgium",
                    "France",
                    "United Kingdom",
                    "UK",
                    "United States",
                    "USA",
                    "Spain",
                    "Italy",
                    "Portugal",
                    "Poland",
                    "Sweden",
                    "Denmark",
                    "Austria",
                    "Switzerland",
                    "Ireland",
                    "Canada",
                    "Australia",
                ]

                # Check if any country is mentioned
                for country in countries:
                    if country.lower() in line.lower():
                        # Split by comma and extract city
                        parts = [p.strip() for p in line.split(",")]
                        if len(parts) >= 2:
                            city = parts[0]
                            # Verify city looks reasonable (not too long, not just numbers)
                            if len(city) > 2 and len(city) < 30 and not city.isdigit():
                                return city, country

        # Check for standalone city names (A4: Expanded city list)
        major_cities = {
            # Dutch cities
            "Amsterdam": "Netherlands",
            "Rotterdam": "Netherlands",
            "Den Haag": "Netherlands",
            "Utrecht": "Netherlands",
            "Eindhoven": "Netherlands",
            "Groningen": "Netherlands",
            # International cities
            "London": "United Kingdom",
            "Berlin": "Germany",
            "Paris": "France",
            "Brussels": "Belgium",
            "New York": "USA",
            "San Francisco": "USA",
        }

        for line in lines[:50]:
            line_clean = line.strip()
            for city, country in major_cities.items():
                if city.lower() in line_clean.lower():
                    # Check if this is likely a location (not part of company name)
                    if len(line_clean) < 50:  # Short line = likely just location
                        return city, country

        return None, None

    def _split_into_sections(self, text: str) -> dict[str, str]:
        """Split resume text into sections.

        Args:
            text: Resume text

        Returns:
            Dict mapping section names to content
        """
        sections = {}

        # Common section headers
        section_patterns = {
            "summary": r"(?i)(professional\s+summary|profile|summary|objective)",
            "experience": r"(?i)(work\s+experience|professional\s+experience|employment|experience)",
            "education": r"(?i)(education|academic|qualifications)",
            "skill": r"(?i)(skills|competencies|expertise)",
            "language": r"(?i)(languages|language\s+skills)",
            "certification": r"(?i)(certifications?|licenses?|credentials?)",
        }

        # Find section positions (use FIRST match of each section only)
        section_positions = []
        found_sections = set()
        for section_key, pattern in section_patterns.items():
            matches = list(re.finditer(pattern, text))
            if matches and section_key not in found_sections:
                # Take only the FIRST match for each section type
                match = matches[0]
                section_positions.append((match.start(), section_key, match.group()))
                found_sections.add(section_key)

        # Sort by position
        section_positions.sort()

        # Extract section content
        for i, (start, key, header) in enumerate(section_positions):
            # Find end of section (next section or end of text)
            if i + 1 < len(section_positions):
                end = section_positions[i + 1][0]
            else:
                end = len(text)

            content = text[start:end].strip()
            # Remove the header line
            content = re.sub(
                f"^{re.escape(header)}", "", content, flags=re.IGNORECASE
            ).strip()

            sections[key] = content

        return sections

    def _extract_work_experience(self, text: str) -> list[WorkExperience]:
        """Extract work experience entries from LinkedIn format.

        LinkedIn uses a hierarchical format:
        Company Name
        Total Duration (X years Y months)
        Position Title 1
        Date Range 1
        Location
        Description...
        Position Title 2 (same company)
        Date Range 2
        Location
        Description...

        Args:
            text: Work experience section text

        Returns:
            List of WorkExperience objects
        """
        experiences = []

        # First, identify company blocks by finding duration summary lines
        # Pattern: "X years Y months" or "X years" or "Y months" on their own line
        duration_summary_pattern = r"^\d+\s+(?:year|years)\s+\d+\s+(?:month|months)$|^\d+\s+(?:year|years)$|^\d+\s+(?:month|months)$"

        lines = text.split("\n")
        company_blocks = []
        current_block_start = 0
        current_company = None

        for i, line in enumerate(lines):
            line_stripped = line.strip()

            # Check if this line is a duration summary
            if re.match(duration_summary_pattern, line_stripped, re.IGNORECASE):
                # Previous line should be the company name
                if i > 0:
                    potential_company = lines[i - 1].strip()

                    # Validate it looks like a company name
                    if (
                        len(potential_company) > 3
                        and not re.match(
                            r"^page\s+\d+", potential_company, re.IGNORECASE
                        )
                        and not re.match(r"^\d+", potential_company)
                    ):
                        # Save previous company block if exists
                        if current_company:
                            company_blocks.append(
                                {
                                    "company": current_company,
                                    "start_line": current_block_start,
                                    "end_line": i - 1,
                                    "text": "\n".join(
                                        lines[current_block_start : i - 1]
                                    ),
                                }
                            )

                        # Start new company block
                        current_company = potential_company
                        current_block_start = i + 1  # Start after duration line

        # Don't forget last company block
        if current_company:
            company_blocks.append(
                {
                    "company": current_company,
                    "start_line": current_block_start,
                    "end_line": len(lines),
                    "text": "\n".join(lines[current_block_start:]),
                }
            )

        # If no company blocks found using duration pattern, fall back to date-based splitting
        if not company_blocks:
            return self._extract_work_experience_fallback(text)

        # Now extract positions from each company block
        date_range_pattern = r"((?:January|February|March|April|May|June|July|August|September|October|November|December|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)\s+\d{4})\s*[-–—]\s*((?:January|February|March|April|May|June|July|August|September|October|November|December|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)\s+\d{4}|Present)"

        for block in company_blocks:
            company_name: str = block["company"]  # type: ignore[assignment]
            block_text: str = block["text"]  # type: ignore[assignment]

            # Split this company's block by date ranges to find individual positions
            parts = re.split(date_range_pattern, block_text, 0, re.IGNORECASE)

            i = 0
            while i < len(parts):
                if i + 3 < len(parts):
                    before_text = parts[i].strip()
                    start_date_str = parts[i + 1].strip()
                    end_date_str = parts[i + 2].strip()
                    after_text = parts[i + 3].strip()

                    # Skip if too short
                    if len(before_text) < 5:
                        i += 1
                        continue

                    exp = WorkExperience()
                    exp.employer = company_name  # Use the identified company name

                    # Parse dates
                    exp.start_date = self._parse_date(start_date_str)
                    if end_date_str.lower() == "present":
                        exp.current = True
                        exp.end_date = None
                    else:
                        exp.end_date = self._parse_date(end_date_str)

                    # Extract position title (last line before date range)
                    lines_before = [
                        line.strip() for line in before_text.split("\n") if line.strip()
                    ]
                    if lines_before:
                        # Skip duration lines
                        position_candidates = [
                            line
                            for line in lines_before
                            if not re.match(
                                r"^\(?\d+\s+(year|month)", line, re.IGNORECASE
                            )
                        ]
                        if position_candidates:
                            exp.position = position_candidates[-1]

                    # Parse location and description from after_text
                    after_lines = [
                        line.strip() for line in after_text.split("\n") if line.strip()
                    ]

                    if after_lines:
                        # Skip duration in parentheses if present
                        if after_lines and re.match(
                            r"^\(?\d+\s+(year|month)", after_lines[0], re.IGNORECASE
                        ):
                            after_lines = after_lines[1:]

                        # Next line might be location
                        if after_lines and (
                            "," in after_lines[0]
                            or any(
                                place in after_lines[0]
                                for place in [
                                    "Netherlands",
                                    "Holland",
                                    "Germany",
                                    "Belgium",
                                    "London",
                                    "New York",
                                    "Amersfoort",
                                    "Utrecht",
                                    "Roosendaal",
                                    "Baarn",
                                    "Hilversum",
                                    "Capelle",
                                    "Gorinchem",
                                ]
                            )
                        ):
                            location_parts = after_lines[0].split(",")
                            if len(location_parts) >= 2:
                                exp.city = location_parts[0].strip()
                                exp.country = location_parts[-1].strip()
                            after_lines = after_lines[1:]

                        # Remaining is description
                        description_lines = []
                        for line in after_lines:
                            # Stop at page markers or next company
                            if re.search(r"^page\s+\d+", line, re.IGNORECASE):
                                break
                            description_lines.append(line)

                        if description_lines:
                            exp.description = "\n".join(description_lines[:20])

                    if exp.position or exp.description:
                        experiences.append(exp)

                    i += 4
                else:
                    i += 1

        return (
            experiences if experiences else self._extract_work_experience_fallback(text)
        )

    def _extract_work_experience_fallback(self, text: str) -> list[WorkExperience]:
        """Fallback work experience extraction when company blocks aren't found."""
        experiences = []

        date_range_pattern = r"((?:January|February|March|April|May|June|July|August|September|October|November|December|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)\s+\d{4})\s*[-–—]\s*((?:January|February|March|April|May|June|July|August|September|October|November|December|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)\s+\d{4}|Present)"
        parts = re.split(date_range_pattern, text, flags=re.IGNORECASE)

        i = 0
        while i + 3 < len(parts):
            before_text = parts[i].strip()
            start_date_str = parts[i + 1].strip()
            end_date_str = parts[i + 2].strip()

            if len(before_text) < 10:
                i += 1
                continue

            exp = WorkExperience()
            exp.start_date = self._parse_date(start_date_str)
            exp.current = end_date_str.lower() == "present"
            exp.end_date = None if exp.current else self._parse_date(end_date_str)

            lines_before = [
                line.strip() for line in before_text.split("\n") if line.strip()
            ]
            if lines_before:
                exp.position = lines_before[-1]
                # Try to find employer
                for line in lines_before[:-1]:
                    if (
                        len(line) > 3
                        and not re.search(r"^\d+\s+(year|month)", line, re.IGNORECASE)
                        and not re.search(r"^page\s+\d+", line, re.IGNORECASE)
                    ):
                        exp.employer = line
                        break

            if exp.position or exp.description:
                experiences.append(exp)

            i += 4

        if not experiences and text.strip():
            experiences.append(WorkExperience(description=text.strip()[:1000]))

        return experiences

    def _parse_date(self, date_str: str) -> Optional[date]:
        """Parse date string to date object.

        Args:
            date_str: Date string like "January 2020" or "Jan 2020"

        Returns:
            date object or None
        """
        from datetime import datetime

        from dateutil import parser

        try:
            # Try to parse with dateutil
            parsed = parser.parse(date_str, default=datetime(2000, 1, 1))
            return parsed.date()
        except Exception:
            # Try to extract at least year and month
            year_match = re.search(r"\b(19|20)\d{2}\b", date_str)
            if year_match:
                year = int(year_match.group())

                month_names = {
                    "jan": 1,
                    "feb": 2,
                    "mar": 3,
                    "apr": 4,
                    "may": 5,
                    "jun": 6,
                    "jul": 7,
                    "aug": 8,
                    "sep": 9,
                    "oct": 10,
                    "nov": 11,
                    "dec": 12,
                }

                for month_name, month_num in month_names.items():
                    if month_name in date_str.lower():
                        from datetime import date as date_class

                        return date_class(year, month_num, 1)

                # Just year
                from datetime import date as date_class

                return date_class(year, 1, 1)

        return None

    def _extract_education(self, text: str) -> list[Education]:
        """Extract education entries.

        Args:
            text: Education section text

        Returns:
            List of Education objects
        """
        education_list = []

        # Common university/school keywords
        institution_keywords = [
            "university",
            "universiteit",
            "college",
            "school",
            "hogeschool",
            "institute",
            "academy",
            "polytechnic",
        ]

        # Split by lines and look for institution names
        lines = text.split("\n")
        current_edu = None
        current_lines = []

        for line in lines:
            line_stripped = line.strip()
            if not line_stripped:
                continue

            # Check if line contains an institution name
            is_institution = any(
                keyword in line_stripped.lower() for keyword in institution_keywords
            )

            # Check if line contains a degree pattern
            degree_patterns = [
                r"bachelor",
                r"master",
                r"phd",
                r"doctorate",
                r"degree",
                r"msc",
                r"bsc",
                r"ma",
                r"ba",
                r"mba",
            ]
            has_degree = any(
                re.search(pattern, line_stripped, re.IGNORECASE)
                for pattern in degree_patterns
            )

            # Start new entry if we find an institution or degree
            if is_institution or (has_degree and not current_edu):
                # Save previous entry if exists
                if current_edu and current_lines:
                    current_edu.description = "\n".join(current_lines[:10])
                    education_list.append(current_edu)

                # Start new entry
                current_edu = Education()
                current_lines = []

                if is_institution:
                    current_edu.organization = line_stripped
                elif has_degree:
                    current_edu.title = line_stripped

            # Accumulate lines for current entry
            if current_edu:
                # Try to extract dates from this line
                date_match = re.search(r"(\d{4})\s*[-–—]\s*(\d{4})", line_stripped)
                if date_match:
                    start_year = int(date_match.group(1))
                    end_year = int(date_match.group(2))
                    from datetime import date as date_class

                    current_edu.start_date = date_class(
                        start_year, 9, 1
                    )  # Assume September start
                    current_edu.end_date = date_class(
                        end_year, 6, 30
                    )  # Assume June end
                elif re.search(r"\b(\d{4})\b", line_stripped):
                    # Single year mention
                    year = int(re.search(r"\b(\d{4})\b", line_stripped).group(1))
                    if not current_edu.end_date:
                        from datetime import date as date_class

                        current_edu.end_date = date_class(year, 6, 30)

                # Check for degree in line if title not set
                if not current_edu.title and has_degree:
                    current_edu.title = line_stripped

                current_lines.append(line_stripped)

        # Don't forget last entry
        if current_edu:
            if current_lines:
                current_edu.description = "\n".join(current_lines[:10])
            education_list.append(current_edu)

        # Fallback: if no structured entries found, create one with all text
        if not education_list and text.strip():
            edu = Education(description=text.strip()[:1000])
            education_list.append(edu)

        return education_list

    def _extract_languages(self, text: str) -> list[Language]:
        """Extract language skills.

        Args:
            text: Language section text

        Returns:
            List of Language objects
        """
        languages = []

        # Common languages
        language_names = [
            "English",
            "Dutch",
            "German",
            "French",
            "Spanish",
            "Italian",
            "Portuguese",
            "Chinese",
            "Japanese",
            "Russian",
            "Arabic",
            "Nederlands",
        ]

        # Proficiency level mappings to CEFR (LinkedIn-specific mappings added)
        proficiency_map = {
            # LinkedIn proficiency levels
            "native or bilingual": "C2",
            "native or bilingual proficiency": "C2",
            "full professional proficiency": "C1",
            "professional working proficiency": "B2",
            "limited working proficiency": "B1",
            "elementary proficiency": "A2",
            # Generic levels
            "native": "C2",
            "bilingual": "C2",
            "fluent": "C1",
            "professional": "C1",
            "advanced": "B2",
            "intermediate": "B1",
            "elementary": "A2",
            "basic": "A1",
            "limited": "B1",
            "full professional": "C1",
            "professional working": "B2",
            "limited working": "B1",
        }

        # CEFR levels
        cefr_pattern = r"\b([A-C][1-2])\b"

        # LinkedIn format: "Language (Proficiency Level)"
        linkedin_pattern = r"(\w+(?:\s+\w+)?)\s*\(([^)]+)\)"
        linkedin_matches = re.finditer(linkedin_pattern, text)

        # Try LinkedIn-specific format first
        for match in linkedin_matches:
            lang_name = match.group(1).strip()
            proficiency = match.group(2).strip()

            # Check if it's a known language
            if any(
                lang.lower() == lang_name.lower() or lang.lower() in lang_name.lower()
                for lang in language_names
            ):
                # Normalize language name to match our list
                normalized_lang = next(
                    (
                        lang
                        for lang in language_names
                        if lang.lower() == lang_name.lower()
                        or lang.lower() in lang_name.lower()
                    ),
                    lang_name,
                )

                language = Language(language=normalized_lang)

                # Map proficiency to CEFR
                prof_lower = proficiency.lower()
                cefr_level = None

                # Try direct CEFR match
                cefr_match = re.search(cefr_pattern, proficiency)
                if cefr_match:
                    cefr_level = cefr_match.group(1)
                else:
                    # Try proficiency mapping
                    for prof_text, level in proficiency_map.items():
                        if prof_text in prof_lower:
                            cefr_level = level
                            break

                if cefr_level:
                    language.listening = cefr_level
                    language.reading = cefr_level
                    language.speaking = cefr_level
                    language.writing = cefr_level

                languages.append(language)

        # Fallback: try generic language detection if no LinkedIn format found
        if not languages:
            for lang in language_names:
                if re.search(rf"\b{lang}\b", text, re.IGNORECASE):
                    language = Language(language=lang)

                    # Find context around the language name (100 chars before and after)
                    lang_pos = text.lower().find(lang.lower())
                    if lang_pos >= 0:
                        context = text[max(0, lang_pos - 100) : lang_pos + 150]

                        # Try to find CEFR level
                        cefr_match = re.search(cefr_pattern, context)
                        if cefr_match:
                            level = cefr_match.group(1)
                            language.listening = level
                            language.reading = level
                            language.speaking = level
                            language.writing = level
                        else:
                            # Try to find proficiency description
                            for prof_text, cefr_level in proficiency_map.items():
                                if re.search(
                                    rf"\b{prof_text}\b", context, re.IGNORECASE
                                ):
                                    language.listening = cefr_level
                                    language.reading = cefr_level
                                    language.speaking = cefr_level
                                    language.writing = cefr_level
                                    break

                    languages.append(language)

        return languages

    def _extract_skills(self, text: str) -> list[Skill]:
        """Extract skills with improved categorization.

        Args:
            text: Skills section text

        Returns:
            List of Skill objects
        """
        skills = []
        seen_skills = set()  # Track duplicates

        # Split by common delimiters
        skill_items = re.split(r"[,•\n·]", text)

        # Noise words to skip (common resume fluff)
        noise_words = {
            "skills",
            "experience",
            "proficient",
            "knowledge",
            "familiar",
            "and",
            "or",
            "including",
            "such as",
            "etc",
            "years",
            "page",
        }

        for item in skill_items:
            item = item.strip()

            # Basic validation
            if not item or len(item) < 2 or len(item) > 50:
                continue

            # Skip if it's just numbers or dates
            if re.match(r"^[\d\s\-/]+$", item):
                continue

            # Skip noise words
            if item.lower() in noise_words:
                continue

            # Skip if contains too many numbers (likely not a skill name)
            if sum(c.isdigit() for c in item) > len(item) // 2:
                continue

            # Normalize for duplicate detection (lowercase, remove spaces)
            normalized = item.lower().replace(" ", "")
            if normalized in seen_skills:
                continue

            seen_skills.add(normalized)
            skill = Skill(name=item)
            skills.append(skill)

        return skills

    def _extract_certifications(self, text: str) -> list[Certification]:
        """Extract certifications.

        Args:
            text: Certifications section text

        Returns:
            List of Certification objects
        """
        certifications = []
        lines = text.split("\n")

        for line in lines:
            line = line.strip()

            # Skip empty lines, page numbers, section headers
            if not line or len(line) < 5:
                continue
            if re.search(r"page\s+\d+|certifications?|licenses?", line, re.IGNORECASE):
                continue

            # Check if line looks like a certification
            # Usually certification lines have capital letters or known cert names
            if any(
                word in line
                for word in [
                    "Certified",
                    "Foundation",
                    "Professional",
                    "AWS",
                    "Azure",
                    "Microsoft",
                ]
            ):
                cert = Certification(name=line)

                # Try to extract date from the line
                year_match = re.search(r"\b(20\d{2})\b", line)
                if year_match:
                    year = int(year_match.group(1))
                    from datetime import date as date_class

                    cert.date = date_class(year, 1, 1)

                certifications.append(cert)

        return certifications
